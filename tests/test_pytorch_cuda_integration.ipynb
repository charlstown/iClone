{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Version: 2.5.1+cu121\n",
      "Cuda Version: 12.1\n",
      "Available Cuda Devices:1\n"
     ]
    }
   ],
   "source": [
    "# Torch library\n",
    "import torch\n",
    "\n",
    "# Checking version and CUDA availability\n",
    "print(f\"Torch Version: {torch.__version__}\")\n",
    "print(f\"Cuda Version: {torch.version.cuda}\")\n",
    "print(f\"Available Cuda Devices:{torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Cambia aquí el modelo que quieras probar\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# Cargando el modelo y el tokenizador\n",
    "def load_model_and_tokenizer(model_name: str):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16  # usa float16 si tu GPU lo soporta\n",
    "    )\n",
    "    return tokenizer, model\n",
    "\n",
    "# Chatbot function\n",
    "def chat(prompt: str, tokenizer, model, max_tokens: int = 150):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_tokens)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face Logging\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.82s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "# Configuración del token de Hugging Face\n",
    "tokenizer, model = load_model_and_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Cual es la capital de Japón?\n",
      "\n",
      "The capital city of Japan is Tokyo. Tokyo is the most populous city in Japan and is located on the eastern coast of the main island of Honshu. It is a major cultural, economic, and political center of Japan and is home to many of Japan's most famous landmarks, including the Sensoji Temple, the Meiji Shrine, and the Tokyo Tower. Tokyo is also known for its vibrant nightlife, delicious food, and advanced technology.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"¿Cual es la capital de Japón?\"\n",
    "response = chat(prompt, tokenizer, model)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "deepnote_notebook_id": "9cf09acbe457448eb7e715ccf1e4d79a",
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
